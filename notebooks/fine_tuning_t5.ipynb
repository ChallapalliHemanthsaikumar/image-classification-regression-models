{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5243eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import os \n",
    "\n",
    "import random\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04af6882",
   "metadata": {},
   "source": [
    "#### Implementing LORA Using PEFT  and open source github tag generator using T5 small \n",
    "\n",
    "what is T5 ?\n",
    "\n",
    "\n",
    "It is NLP model for text to text problem , meaning both the input and output are text strings \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "390f3bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LORA is Low Rank Adaptation  is a parameter efficient fine tuning method or technique for large language models (LLMs) that aims to reduce the number of trainable parameters while maintaining performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dad67714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import userdata\n",
    "# import os\n",
    "\n",
    "# os.environ[\"HUGGINGFACE_TOKEN\"] = userdata.get(\"HUGGINGFACE_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482435bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers datasets peft bitsandbytes -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b592974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Kavitha padala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0379543d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\kavitha padala\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.36.0)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.55.3-py3-none-any.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     -------------------------------------  41.0/42.0 kB 991.0 kB/s eta 0:00:01\n",
      "     -------------------------------------- 42.0/42.0 kB 689.8 kB/s eta 0:00:00\n",
      "Requirement already satisfied: accelerate in c:\\users\\kavitha padala\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.26.0)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.10.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\kavitha padala\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\kavitha padala\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kavitha padala\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kavitha padala\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kavitha padala\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\kavitha padala\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.4-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\kavitha padala\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\kavitha padala\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\kavitha padala\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\kavitha padala\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (2.4.1+cu118)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\kavitha padala\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\kavitha padala\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\kavitha padala\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\kavitha padala\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kavitha padala\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\kavitha padala\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kavitha padala\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kavitha padala\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kavitha padala\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kavitha padala\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kavitha padala\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\kavitha padala\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Downloading transformers-4.55.3-py3-none-any.whl (11.3 MB)\n",
      "   ---------------------------------------- 0.0/11.3 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.5/11.3 MB 46.9 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 3.1/11.3 MB 39.9 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.3/11.3 MB 42.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 7.8/11.3 MB 41.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.2/11.3 MB 43.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.3/11.3 MB 46.7 MB/s eta 0:00:00\n",
      "Downloading accelerate-1.10.0-py3-none-any.whl (374 kB)\n",
      "   ---------------------------------------- 0.0/374.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 374.7/374.7 kB ? eta 0:00:00\n",
      "Downloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "   ---------------------------------------- 0.0/561.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 561.5/561.5 kB ? eta 0:00:00\n",
      "Downloading tokenizers-0.21.4-cp39-abi3-win_amd64.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.5/2.5 MB 80.6 MB/s eta 0:00:00\n",
      "Installing collected packages: huggingface-hub, tokenizers, accelerate, transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.26.2\n",
      "    Uninstalling huggingface-hub-0.26.2:\n",
      "      Successfully uninstalled huggingface-hub-0.26.2\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.2\n",
      "    Uninstalling tokenizers-0.15.2:\n",
      "      Successfully uninstalled tokenizers-0.15.2\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.26.0\n",
      "    Uninstalling accelerate-0.26.0:\n",
      "      Successfully uninstalled accelerate-0.26.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.36.0\n",
      "    Uninstalling transformers-4.36.0:\n",
      "      Successfully uninstalled transformers-4.36.0\n",
      "Successfully installed accelerate-1.10.0 huggingface-hub-0.34.4 tokenizers-0.21.4 transformers-4.55.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Kavitha padala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\~~kenizers'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spacy-transformers 1.3.5 requires transformers<4.37.0,>=3.4.0, but you have transformers 4.55.3 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# In a notebook cell:\n",
    "# !pip install --upgrade transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a263b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b893b5be5f4a466a9d818b33c3fa9d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kavitha padala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Kavitha padala\\.cache\\huggingface\\hub\\models--t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75279bba73a645b0a14b393cd68700b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68c54dcf5aff432fa77da599f426b63f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1ae5cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be6361dc2ab843deb67946bfc5cc1800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kavitha padala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Kavitha padala\\.cache\\huggingface\\hub\\datasets--zamal--github-meta-data. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "163752844227489da0228a2c493cfeb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/32.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2041a7a070b247338be835af8e5f3c19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/614 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load existing dataset with only a \"train\" split\n",
    "from datasets import  DatasetDict\n",
    "\n",
    "dataset = load_dataset(\"zamal/github-meta-data\")  # returns DatasetDict\n",
    "\n",
    "# Split the train set into train and validation\n",
    "split = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "# Wrap into a new DatasetDict\n",
    "dataset_dict = DatasetDict({\"train\": split[\"train\"], \"validation\": split[\"test\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "010019e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'typescript css library postcss tailwindcss',\n",
       " 'target': 'tailwindcss, css, css-framework, postcss, functional-css, utility-classes, responsive'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b9e5211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552\n",
      "62\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset_dict[\"train\"]))\n",
    "print(len(dataset_dict[\"validation\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0f38902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f6df375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(batch):\n",
    "    inputs = batch[\"input\"]\n",
    "    targets = batch[\"target\"]\n",
    "    model_inputs = tokenizer(inputs, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    labels = tokenizer(targets, return_tensors=\"pt\", padding=True, truncation=True).input_ids\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3738462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb1e579fa157416b864d5680dae5b8d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/552 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8134c93fec748dcb570a921fbc2d89a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/62 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized = dataset_dict.map(preprocess, batched=True, remove_columns=dataset_dict[\"train\"].column_names)\n",
    "tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fab17a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  686, 11815,     3,    75,     7,     7,  3595,   442,    75,     7,\n",
       "             7,  9891,  5165,    75,     7,     7,     1,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'labels': tensor([ 9891,  5165,    75,     7,     7,     6,     3,    75,     7,     7,\n",
       "             6,     3,    75,     7,     7,    18, 11415,  1981,     6,   442,\n",
       "            75,     7,     7,     6,  5014,    18,    75,     7,     7,     6,\n",
       "          6637,    18,  4057,    15,     7,     6, 10586,     1,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e1394a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d30a39d87dc444258f4c1c4651d8a3e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "983aae97e4374161a2241e931784e63b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97ba8736f5d247a884ae05a883743f32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d5c5be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],  # Adjust based on model architecture\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed650e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d9d7905",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./t5_tag_generator\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=25,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_steps=50,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=\"zamal/github-tag-generatorr\",  # Replace with your Hugging Face username\n",
    "    hub_token=os.environ[\"HUGGINGFACE_TOKEN\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7925233",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kavitha padala\\AppData\\Local\\Temp\\ipykernel_19840\\2744959247.py:6: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97a6ee09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kavitha padala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\data\\data_collator.py:741: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1725' max='1725' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1725/1725 02:27, Epoch 25/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>8.405300</td>\n",
       "      <td>7.104400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.423500</td>\n",
       "      <td>3.334789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.811000</td>\n",
       "      <td>2.331543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.509200</td>\n",
       "      <td>2.101823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.183000</td>\n",
       "      <td>1.991302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.935900</td>\n",
       "      <td>1.901142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.739500</td>\n",
       "      <td>1.805873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.658900</td>\n",
       "      <td>1.712887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.588900</td>\n",
       "      <td>1.639096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.652300</td>\n",
       "      <td>1.581841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.523000</td>\n",
       "      <td>1.538155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.472900</td>\n",
       "      <td>1.495742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.587700</td>\n",
       "      <td>1.458611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.427100</td>\n",
       "      <td>1.430735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.301700</td>\n",
       "      <td>1.411384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.382800</td>\n",
       "      <td>1.398594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.324200</td>\n",
       "      <td>1.384050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.375300</td>\n",
       "      <td>1.369368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.304200</td>\n",
       "      <td>1.357473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.425500</td>\n",
       "      <td>1.346572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.255800</td>\n",
       "      <td>1.336254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.461500</td>\n",
       "      <td>1.326780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.302900</td>\n",
       "      <td>1.323322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.231700</td>\n",
       "      <td>1.316327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.399600</td>\n",
       "      <td>1.311856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.246200</td>\n",
       "      <td>1.306408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.146700</td>\n",
       "      <td>1.303645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.346500</td>\n",
       "      <td>1.300887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>1.271000</td>\n",
       "      <td>1.296413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.260400</td>\n",
       "      <td>1.294602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>1.210000</td>\n",
       "      <td>1.292938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.277800</td>\n",
       "      <td>1.291710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>1.288900</td>\n",
       "      <td>1.290990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.231600</td>\n",
       "      <td>1.290092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1725, training_loss=1.8801535841347514, metrics={'train_runtime': 148.5452, 'train_samples_per_second': 92.901, 'train_steps_per_second': 11.613, 'total_flos': 303131197440000.0, 'train_loss': 1.8801535841347514, 'epoch': 25.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44d70bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def generate_tags(query, model, tokenizer, max_length=64, num_beams=5):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128).to(\n",
    "        model.device\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=True,\n",
    "            decoder_start_token_id=tokenizer.pad_token_id,  # üëà required for T5\n",
    "        )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2a6b153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'image augmentation, no code implementations, css, js, js, css, css, js'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_tags(\"looking for repositories on image augmentation no code implementations,html and css and js \", model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00ea0851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model and tokenizer saved locally at: ../models/t5_tag_generator\n"
     ]
    }
   ],
   "source": [
    "# Save model, tokenizer, and config to local output directory\n",
    "model_path = \"../models/t5_tag_generator\"\n",
    "\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n",
    "\n",
    "print(\"‚úÖ Model and tokenizer saved locally at:\", model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2eb27f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Input: Need an AI tool to convert customer voice calls into structured CRM record\n",
      "üè∑Ô∏è Tags: voice-calls, CRM-records, voice-calls\n",
      "\n",
      "üì• Input: How to train a text summarization model using Pegasus or BART\n",
      "üè∑Ô∏è Tags: text summarization, pegasus, bART, et al.\n",
      "\n",
      "üì• Input: Fine-tuning BERT for spam detection in emails\n",
      "üè∑Ô∏è Tags: BERT, spam-detecting, email-tuning, email-tuning, email-tuning\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = [\n",
    "    \"Need an AI tool to convert customer voice calls into structured CRM record\",\n",
    "    \"How to train a text summarization model using Pegasus or BART\",\n",
    "    \"Fine-tuning BERT for spam detection in emails\",\n",
    "]\n",
    "\n",
    "for text in inputs:\n",
    "    print(f\"üì• Input: {text}\")\n",
    "    print(f\"üè∑Ô∏è Tags: {generate_tags(text, model, tokenizer)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6dacf696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tags(text, model, tokenizer, max_length=64, num_beams=5):\n",
    "    input_text = text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128).to(\n",
    "        model.device\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=True,\n",
    "            decoder_start_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Deduplicate and clean tags\n",
    "    tags = [t.strip().lower() for t in decoded.split(\",\")]\n",
    "    unique_tags = list(dict.fromkeys(tags))  # preserve order + remove duplicates\n",
    "    return \", \".join(unique_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b28c5012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Input: Need an AI tool to convert customer voice calls into structured CRM record\n",
      "üè∑Ô∏è Tags: voice-calls, crm-records\n",
      "\n",
      "üì• Input: How to train a text summarization model using Pegasus or BART\n",
      "üè∑Ô∏è Tags: text summarization, pegasus, bart, et al.\n",
      "\n",
      "üì• Input: Fine-tuning BERT for spam detection in emails\n",
      "üè∑Ô∏è Tags: bert, spam-detecting, email-tuning\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = [\n",
    "    \"Need an AI tool to convert customer voice calls into structured CRM record\",\n",
    "    \"How to train a text summarization model using Pegasus or BART\",\n",
    "    \"Fine-tuning BERT for spam detection in emails\",\n",
    "]\n",
    "\n",
    "for text in inputs:\n",
    "    print(f\"üì• Input: {text}\")\n",
    "    print(f\"üè∑Ô∏è Tags: {generate_tags(text, model, tokenizer)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c8b871",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
